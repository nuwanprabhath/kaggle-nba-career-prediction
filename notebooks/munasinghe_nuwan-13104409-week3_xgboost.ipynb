{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1a09faa8-353f-4088-8eb7-e8e2a0ae0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# For Hyperparameter optimization\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "\n",
    "# TODO Jupyter working directory is /notebooks. Therefore importing \"from src.data\" isn't working. Manually passing --notebook-dir\n",
    "# when running the notebook didn't work. Needs to be investigated and fixed. This is a workaround\n",
    "process_test_data = SourceFileLoader('process_test_data', '../src/data/process_test_data.py').load_module()\n",
    "visualize = SourceFileLoader('visualize', '../src/visualization/visualize.py').load_module()\n",
    "\n",
    "# True if you want to run with the Kaggle train and test set for submission. Otherwise it will use split kaggle train data for model \n",
    "# optimization to calculate stats and parameter optimization\n",
    "use_kaggle_data = False \n",
    "run_parameter_optimization = True\n",
    "run_feature_selection = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f35d8-68cc-4157-a4cb-454c3d385e2d",
   "metadata": {},
   "source": [
    "If not using Kaggle data set for submission, split train datasets for training (80%), testing (10%) and validation (10%)\n",
    "and normalize features using MinMaxScaler. Else load full Kaggle data and predict using Kaggle test set for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "56538e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train shape: (8000, 21)\n",
      "Concat shape: (8000, 20)\n",
      "Files written to: ../data/processed\n",
      "X_train shape: (6400, 19)\n",
      "y_train shape: (6400, 1)\n",
      "X_test shape: (800, 20)\n",
      "y_test shape: (800, 1)\n",
      "X_valid shape: (800, 20)\n",
      "y_valid shape: (800, 1)\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data:\n",
    "    X_train, y_train, X_test = process_test_data.load_kaggle_train_and_test_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
    "else:    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = \\\n",
    "    process_test_data.split_and_normalize('../data/raw/train.csv', '../data/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70bc90",
   "metadata": {},
   "source": [
    "Check details of the data if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "773201bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1086e3",
   "metadata": {},
   "source": [
    "Selecting features using sequential feature selection if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cc1e141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GP' 'MIN' 'PTS' 'FGM' 'FGA' 'FG%' '3P Made' '3PA' 'OREB' 'DREB' 'REB'\n",
      " 'BLK' 'TOV']\n"
     ]
    }
   ],
   "source": [
    "if run_feature_selection==True:\n",
    "    num_of_features_to_select = 13\n",
    "    features =  process_test_data.sequential_feature_selection('../data/raw/train.csv', num_of_features_to_select) # ['GP', 'MIN', 'FGM', '3P Made', 'OREB', 'BLK', 'TOV']\n",
    "    X_train = X_train[features]\n",
    "    # Appending Id column since it should be kept\n",
    "    features.append('Id')\n",
    "    X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fe867-667d-4865-ae47-eb6ce0781730",
   "metadata": {},
   "source": [
    "Running parameter optimization if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "37df6ab0-d6a8-401f-98fe-134e0b417211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 100/100 [10:21<00:00,  6.22s/trial, best loss: 0.2980653739630136]\n",
      "Best: {'colsample_bytree': 0.8, 'grow_policy': 1, 'learning_rate': 0.2, 'max_delta_step': 3, 'max_depth': 10, 'min_child_weight': 9.0, 'min_split_loss': 13, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Defining pre-identified best parameters if parameter optimization is not going to run\n",
    "hyp_params = { # kaggle score of 0.70342, with 13 features\n",
    "'colsample_bytree': 0.75,\n",
    "'grow_policy': 'lossguide',\n",
    "'learning_rate': 0.2,\n",
    "'max_delta_step': 2,\n",
    "'max_depth': 1, \n",
    "'min_child_weight': 2,\n",
    "'min_split_loss': 12,\n",
    "'subsample': 0.95,\n",
    "'booster': 'gbtree'     \n",
    "}\n",
    "\n",
    "def hyperparameter_tuning(space):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(**space, eval_metric=\"auc\", use_label_encoder=False) \n",
    "    acc = cross_val_score(xgboost, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "    return{'loss': 1-acc, 'status': STATUS_OK }\n",
    "\n",
    "if run_parameter_optimization==True:\n",
    "    booster = ['gbtree', 'gblinear', 'dart']\n",
    "    grow_policy = ['depthwise', 'lossguide']\n",
    "    \n",
    "    space = {\n",
    "    \"max_depth\": hp.choice('max_depth', range(5, 20, 1)),\n",
    "    \"learning_rate\": hp.quniform('learning_rate', 0.01, 0.5, 0.05),\n",
    "    \"min_child_weight\": hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    \"subsample\": hp.quniform('subsample', 0.1, 1, 0.05),\n",
    "    \"colsample_bytree\": hp.quniform('colsample_bytree', 0.1, 1.0, 0.05),\n",
    "    \"min_split_loss\": hp.choice('min_split_loss', range(0, 20, 1)),\n",
    "    \"max_delta_step\": hp.choice('max_delta_step', range(0, 10, 1)),\n",
    "    \"grow_policy\": hp.choice(\"grow_policy\", grow_policy),     \n",
    "    \"booster\": booster[0]     \n",
    "    }\n",
    "    \n",
    "    # Initialize trials object\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn=hyperparameter_tuning, space = space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    \n",
    "    hyp_params['max_depth'] = best['max_depth']\n",
    "    hyp_params['learning_rate'] = best['learning_rate']\n",
    "    hyp_params['min_child_weight'] = best['min_child_weight']\n",
    "    hyp_params['subsample'] = best['subsample']\n",
    "    hyp_params['colsample_bytree'] = best['colsample_bytree'] \n",
    "    hyp_params['min_split_loss'] = best['min_split_loss']\n",
    "    hyp_params['max_delta_step'] = best['max_delta_step']\n",
    "    hyp_params['grow_policy'] = grow_policy[best['grow_policy']]\n",
    "    hyp_params['booster'] = booster[0]\n",
    "    \n",
    "    print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73c16f-e5b8-4adf-a274-b71205accd7b",
   "metadata": {},
   "source": [
    "Training the xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1a837069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8,\n",
       "              enable_categorical=False, gamma=13, gpu_id=-1,\n",
       "              grow_policy='lossguide', importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.2, max_delta_step=3,\n",
       "              max_depth=10, min_child_weight=9.0, min_split_loss=13,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=8, num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, ...)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = xgb.XGBClassifier(**hyp_params, use_label_encoder=False)\n",
    "\n",
    "# Converting column y values to 1d array\n",
    "xgboost.fit(X_train, y_train, eval_metric='auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354c22b",
   "metadata": {},
   "source": [
    "Predicting using trained random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "567ad90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns to train\n",
    "test_X = X_test.loc[:, 'GP':'TOV']\n",
    "# Selecting Ids for CSV\n",
    "test_X_Ids = X_test.loc[:,'Id']\n",
    "\n",
    "if use_kaggle_data==True:\n",
    "    # Predicting probabilities for kaggle submission and selecting probability of class 1.\n",
    "    pred = xgboost.predict_proba(test_X)[:,1]  \n",
    "else:\n",
    "    # Predicting classes (1 or 0) for calculating accuracy\n",
    "    pred = xgboost.predict(test_X) \n",
    "    # Probabilities for calculating ROC\n",
    "    rf_probs = xgboost.predict_proba(test_X)[:,1]\n",
    "\n",
    "# Data frame with ID for csv writing. In Kaggle mode pred will contains probabilities and else contains classes\n",
    "result = pd.DataFrame(data = {'Id': test_X_Ids, 'TARGET_5Yrs': pred}) \n",
    "# Extracting values for calculating stats\n",
    "result_values = result[['TARGET_5Yrs']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee49e4",
   "metadata": {},
   "source": [
    "Saving the trainned model and writing result to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b2ec11c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nuwan_xgboost_v16.joblib']"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgboost, \"../models/nuwan_xgboost_v16.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c12746",
   "metadata": {},
   "source": [
    "Show stats related to performance of the model if not using Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "214fb000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error: 16.874999999999996%\n",
      "ROC: 0.70496\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data==False:\n",
    "    visualize.show_random_forest_stats(xgboost, test_X, y_test, rf_probs)\n",
    "    # visualize.show_feature_importance(rf, X_train) # Uncomment to see feature importance if required\n",
    "else:\n",
    "    result.to_csv(\"../data/external/submission_nuwan_v16.csv\", index = False)\n",
    "    print(\"Kaggle dataset and no stats. Writing to a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9d985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd8d2c4ed1a7f8449a8ed3fc82c2ff6cc1ec75d26fd7e344fadf328bd613336a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
