{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1a09faa8-353f-4088-8eb7-e8e2a0ae0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from importlib.machinery import SourceFileLoader\n",
    "# For Hyperparameter optimization\n",
    "from hpsklearn import HyperoptEstimator\n",
    "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
    "\n",
    "# TODO Jupyter working directory is /notebooks. Therefore importing \"from src.data\" isn't working. Manually passing --notebook-dir\n",
    "# when running the notebook didn't work. Needs to be investigated and fixed. This is a workaround\n",
    "process_test_data = SourceFileLoader('process_test_data', '../src/data/process_test_data.py').load_module()\n",
    "visualize = SourceFileLoader('visualize', '../src/visualization/visualize.py').load_module()\n",
    "\n",
    "# True if you want to run with the Kaggle train and test set for submission. Otherwise it will use split kaggle train data for model \n",
    "# optimization to calculate stats and parameter optimization\n",
    "use_kaggle_data = False \n",
    "run_parameter_optimization = True\n",
    "run_feature_selection = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f35d8-68cc-4157-a4cb-454c3d385e2d",
   "metadata": {},
   "source": [
    "If not using Kaggle data set for submission, split train datasets for training (80%), testing (10%) and validation (10%)\n",
    "and normalize features using MinMaxScaler. Else load full Kaggle data and predict using Kaggle test set for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "56538e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train shape: (8000, 21)\n",
      "Concat shape: (8000, 20)\n",
      "Files written to: ../data/processed\n",
      "X_train shape: (6400, 19)\n",
      "y_train shape: (6400, 1)\n",
      "X_test shape: (800, 20)\n",
      "y_test shape: (800, 1)\n",
      "X_valid shape: (800, 20)\n",
      "y_valid shape: (800, 1)\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data:\n",
    "    X_train, y_train, X_test = process_test_data.load_kaggle_train_and_test_data('../data/raw/train.csv', '../data/raw/test.csv')\n",
    "else:    \n",
    "    X_train, y_train, X_test, y_test, X_valid, y_valid = \\\n",
    "    process_test_data.split_and_normalize('../data/raw/train.csv', '../data/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70bc90",
   "metadata": {},
   "source": [
    "Check details of the data if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "773201bf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1086e3",
   "metadata": {},
   "source": [
    "Selecting features using sequential feature selection if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cc1e141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_feature_selection==True:\n",
    "    num_of_features_to_select = 7\n",
    "    features = process_test_data.sequential_feature_selection('../data/raw/train.csv', num_of_features_to_select) # ['GP', 'MIN', 'FGM', '3P Made', 'OREB', 'BLK', 'TOV']\n",
    "    X_train = X_train[features]\n",
    "    # Appending Id column since it should be kept\n",
    "    features.append('Id')\n",
    "    X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fe867-667d-4865-ae47-eb6ce0781730",
   "metadata": {},
   "source": [
    "Running parameter optimization if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "37df6ab0-d6a8-401f-98fe-134e0b417211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [1:06:40<00:00, 40.00s/trial, best loss: -0.8359375]\n",
      "Best: {'class_weight': 2, 'criterion': 1, 'max_depth': 28.0, 'max_features': 1, 'n_estimators': 7}\n"
     ]
    }
   ],
   "source": [
    "# Defining pre-identified best parameters if parameter optimization is not going to run\n",
    "hyp_params = {\n",
    "'n_estimators': 200, \n",
    "'max_depth': 12, \n",
    "'criterion': 'entropy',\n",
    "'class_weight': None,\n",
    "'max_features': 'auto'\n",
    "}\n",
    "\n",
    "def hyperparameter_tuning(params):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    clf = RandomForestClassifier(**params, n_jobs=-1)\n",
    "    acc = cross_val_score(clf, X_train, y_train.values.ravel(), scoring=\"accuracy\", cv=10).mean()\n",
    "    return {\"loss\": -acc, \"status\": STATUS_OK}\n",
    "\n",
    "if run_parameter_optimization==True:\n",
    "    n_estimators=[100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "    criterian=[\"gini\", \"entropy\"]\n",
    "    class_weight=[\"balanced_subsample\", \"balanced\", None]\n",
    "    max_features=[\"auto\", \"sqrt\", \"log2\"]\n",
    "    space = {\n",
    "    \"n_estimators\": hp.choice(\"n_estimators\", n_estimators),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 1, 30,1),\n",
    "    \"criterion\": hp.choice(\"criterion\", [\"gini\", \"entropy\"]),\n",
    "    \"class_weight\": hp.choice(\"class_weight\", class_weight),\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features),    \n",
    "    }\n",
    "    \n",
    "    # Initialize trials object\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn=hyperparameter_tuning, space = space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    \n",
    "    hyp_params['n_estimators'] = n_estimators[best['n_estimators']]\n",
    "    hyp_params['max_depth'] = best['max_depth']\n",
    "    hyp_params['criterion'] = criterian[best['criterion']]\n",
    "    hyp_params['class_weight'] = class_weight[best['class_weight']]\n",
    "    hyp_params['max_features'] = max_features[best['max_features']]\n",
    "    \n",
    "    print(\"Best: {}\".format(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73c16f-e5b8-4adf-a274-b71205accd7b",
   "metadata": {},
   "source": [
    "Training the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1a837069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=28.0, max_features='sqrt',\n",
       "                       n_estimators=800, n_jobs=1, oob_score=True,\n",
       "                       random_state=44)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=hyp_params['n_estimators'], n_jobs=1, random_state = 44, max_features=hyp_params['max_features'], \\\n",
    "                            oob_score=True, class_weight=hyp_params['class_weight'], max_depth=hyp_params['max_depth'], criterion=hyp_params['criterion'])\n",
    "\n",
    "# Converting column y values to 1d array\n",
    "rf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8354c22b",
   "metadata": {},
   "source": [
    "Predicting using trained random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "567ad90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns to train\n",
    "test_X = X_test.loc[:, 'GP':'TOV']\n",
    "# Selecting Ids for CSV\n",
    "test_X_Ids = X_test.loc[:,'Id']\n",
    "\n",
    "if use_kaggle_data==True:\n",
    "    # Predicting probabilities for kaggle submission and selecting probability of class 1.\n",
    "    pred = rf.predict_proba(test_X)[:,1]  \n",
    "else:\n",
    "    # Predicting classes (1 or 0) for calculating accuracy\n",
    "    pred = rf.predict(test_X) \n",
    "    # Probabilities for calculating ROC\n",
    "    rf_probs = rf.predict_proba(test_X)[:,1]\n",
    "\n",
    "# Data frame with ID for csv writing. In Kaggle mode pred will contains probabilities and else contains classes\n",
    "result = pd.DataFrame(data = {'Id': test_X_Ids, 'TARGET_5Yrs': pred}) \n",
    "# Extracting values for calculating stats\n",
    "result_values = result[['TARGET_5Yrs']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee49e4",
   "metadata": {},
   "source": [
    "Saving the trainned model and writing result to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b2ec11c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nuwan_random_forest_v13.joblib']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf, \"../models/nuwan_random_forest_v13.joblib\", compress=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c12746",
   "metadata": {},
   "source": [
    "Show stats related to performance of the model if not using Kaggle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "214fb000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average absolute error: 16.75%\n",
      "ROC: 0.69202\n"
     ]
    }
   ],
   "source": [
    "if use_kaggle_data==False:\n",
    "    visualize.show_random_forest_stats(rf, test_X, y_test, rf_probs)\n",
    "    # visualize.show_feature_importance(rf, X_train) # Uncomment to see feature importance if required\n",
    "else:\n",
    "    result.to_csv(\"../data/external/submission_nuwan_v13.csv\", index = False)\n",
    "    print(\"Kaggle dataset and no stats. Writing to a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9d985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd8d2c4ed1a7f8449a8ed3fc82c2ff6cc1ec75d26fd7e344fadf328bd613336a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
